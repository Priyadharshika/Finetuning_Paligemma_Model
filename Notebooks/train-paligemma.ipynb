{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T11:30:48.579392Z",
     "iopub.status.busy": "2025-03-11T11:30:48.579122Z",
     "iopub.status.idle": "2025-03-11T11:31:58.739093Z",
     "shell.execute_reply": "2025-03-11T11:31:58.738008Z",
     "shell.execute_reply.started": "2025-03-11T11:30:48.579364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hFound existing installation: bitsandbytes 0.45.3\n",
      "Uninstalling bitsandbytes-0.45.3:\n",
      "  Successfully uninstalled bitsandbytes-0.45.3\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Using cached bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers, bitsandbytes, accelerate\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.2.1\n",
      "    Uninstalling accelerate-1.2.1:\n",
      "      Successfully uninstalled accelerate-1.2.1\n",
      "Successfully installed accelerate-1.4.0 bitsandbytes-0.45.3 transformers-4.49.0\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets lightning\n",
    "!pip install -q peft accelerate bitsandbytes\n",
    "!pip install -q --upgrade wandb\n",
    "\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -U bitsandbytes transformers accelerate\n",
    "!pip install huggingface_hub  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T11:58:11.795672Z",
     "iopub.status.busy": "2025-03-11T11:58:11.795350Z",
     "iopub.status.idle": "2025-03-11T11:58:11.904150Z",
     "shell.execute_reply": "2025-03-11T11:58:11.903312Z",
     "shell.execute_reply.started": "2025-03-11T11:58:11.795647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightning as L\n",
    "import re\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "from transformers import PaliGemmaForConditionalGeneration\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Any, List, Dict\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\"xxxxxxxxxxxxx\")\n",
    "\n",
    "import wandb\n",
    "wandb.init(mode=\"offline\")\n",
    "\n",
    "\n",
    "\n",
    "# Configuration Constants\n",
    "MAX_LENGTH = 512\n",
    "REPO_ID = \"google/paligemma-3b-mix-224\"\n",
    "#FINETUNED_MODEL_ID = \"xxxxx\"\n",
    "PROMPT = \"extract JSON.\"\n",
    "WANDB_PROJECT = \"paligemma_finetuning\"\n",
    "WANDB_NAME = \"paligemma_train\"\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        split: str = \"train\",\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                gt_jsons = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        gt_json,\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    for gt_json in gt_jsons  # load json from list of json\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def json2token(self, obj: Any, sort_json_key: bool = True):\n",
    "        # Convert an ordered JSON object into a token sequence..\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            return obj\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "    \n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        image = sample[\"image\"]\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
    "        print(target_sequence)\n",
    "        return image, target_sequence\n",
    "\n",
    "class PaliGemmaModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "        # Initialize lists to track losses for plotting\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, token_type_ids, attention_mask, pixel_values, labels = batch\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids,\n",
    "                             pixel_values=pixel_values,\n",
    "                             labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        # Append to train_losses list for later plotting\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "        # Log to file\n",
    "        with open(\"training_loss.txt\", \"a\") as f:\n",
    "            f.write(f\"Train Loss (Epoch {self.current_epoch}): {loss.item()}\\n\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        input_ids, attention_mask, pixel_values, answers = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                            pixel_values=pixel_values, max_new_tokens=MAX_LENGTH)\n",
    "        # turn them back into text, chopping off the prompt\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        val_loss = np.mean(scores)\n",
    "        self.log(\"val_edit_distance\", val_loss)\n",
    "\n",
    "        # Append to val_losses list for later plotting\n",
    "        self.val_losses.append(val_loss)\n",
    "\n",
    "        # Log to file\n",
    "        with open(\"validation_loss.txt\", \"a\") as f:\n",
    "            f.write(f\"Validation Loss (Epoch {self.current_epoch}): {val_loss}\\n\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # You could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Plot the losses at the end of each epoch\n",
    "        if self.current_epoch % 1 == 0:  # Change the modulo if you want to plot after specific epochs\n",
    "            self.plot_losses()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        # Plot the training and validation losses\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label=\"Training Loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"loss_plot_epoch_{self.current_epoch}.png\")  # Save plot as image\n",
    "        plt.show() \n",
    "        \n",
    "\n",
    "# Initialize Processor and Model\n",
    "def initialize_processor_and_model():\n",
    "    processor = AutoProcessor.from_pretrained(REPO_ID)\n",
    "    model = PaliGemmaForConditionalGeneration.from_pretrained(REPO_ID)\n",
    "    return processor, model\n",
    "\n",
    "# Data Loading\n",
    "def load_data():\n",
    "    dataset = load_dataset(\"naver-clova-ix/cord-v2\")\n",
    "    train_dataset = CustomDataset(\"naver-clova-ix/cord-v2\", split=\"train\")\n",
    "    val_dataset = CustomDataset(\"naver-clova-ix/cord-v2\", split=\"validation\")\n",
    "    return dataset, train_dataset, val_dataset\n",
    "\n",
    "# Save Model & Processor\n",
    "def save_model(model_module, save_directory):\n",
    "    model_module.model.save_pretrained(save_directory)\n",
    "    processor.save_pretrained(save_directory)\n",
    "    zip_path = f\"{save_directory}.zip\"\n",
    "    shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', save_directory)\n",
    "    return zip_path\n",
    "\n",
    "def train_collate_fn(examples):\n",
    "  images = [example[0] for example in examples]\n",
    "  print(images)\n",
    "  texts = [PROMPT for _ in range(len(images))]\n",
    "  print(texts)\n",
    "  labels = [example[1] for example in examples]\n",
    "  print(labels)\n",
    "\n",
    "  inputs = processor(text=texts, images=images, suffix=labels, return_tensors=\"pt\", padding=True,\n",
    "                     truncation=\"only_second\", max_length=MAX_LENGTH,\n",
    "                     tokenize_newline_separately=False)\n",
    "\n",
    "  input_ids = inputs[\"input_ids\"]\n",
    "  token_type_ids = inputs[\"token_type_ids\"]\n",
    "  attention_mask = inputs[\"attention_mask\"]\n",
    "  pixel_values = inputs[\"pixel_values\"]\n",
    "  labels = inputs[\"labels\"]\n",
    "\n",
    "  return input_ids, token_type_ids, attention_mask, pixel_values, labels\n",
    "\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "  images = [example[0] for example in examples]\n",
    "  texts = [PROMPT for _ in range(len(images))]\n",
    "  answers = [example[1] for example in examples]\n",
    "\n",
    "  inputs = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, tokenize_newline_separately=False)\n",
    "\n",
    "  input_ids = inputs[\"input_ids\"]\n",
    "  attention_mask = inputs[\"attention_mask\"]\n",
    "  pixel_values = inputs[\"pixel_values\"]\n",
    "\n",
    "  return input_ids, attention_mask, pixel_values, answers\n",
    "\n",
    "\n",
    "def token2json(tokens, is_inner_value=False, added_vocab=None):\n",
    "        \"\"\"\n",
    "        Convert a (generated) token sequence into an ordered JSON format.\n",
    "        \"\"\"\n",
    "        if added_vocab is None:\n",
    "            added_vocab = processor.tokenizer.get_added_vocab()\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        while tokens:\n",
    "            start_token = re.search(r\"<s_(.*?)>\", tokens, re.IGNORECASE)\n",
    "            if start_token is None:\n",
    "                break\n",
    "            key = start_token.group(1)\n",
    "            key_escaped = re.escape(key)\n",
    "\n",
    "            end_token = re.search(rf\"</s_{key_escaped}>\", tokens, re.IGNORECASE)\n",
    "            start_token = start_token.group()\n",
    "            if end_token is None:\n",
    "                tokens = tokens.replace(start_token, \"\")\n",
    "            else:\n",
    "                end_token = end_token.group()\n",
    "                start_token_escaped = re.escape(start_token)\n",
    "                end_token_escaped = re.escape(end_token)\n",
    "                content = re.search(\n",
    "                    f\"{start_token_escaped}(.*?){end_token_escaped}\", tokens, re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                if content is not None:\n",
    "                    content = content.group(1).strip()\n",
    "                    if r\"<s_\" in content and r\"</s_\" in content:  # non-leaf node\n",
    "                        value = token2json(content, is_inner_value=True, added_vocab=added_vocab)\n",
    "                        if value:\n",
    "                            if len(value) == 1:\n",
    "                                value = value[0]\n",
    "                            output[key] = value\n",
    "                    else:  # leaf nodes\n",
    "                        output[key] = []\n",
    "                        for leaf in content.split(r\"<sep/>\"):\n",
    "                            leaf = leaf.strip()\n",
    "                            if leaf in added_vocab and leaf[0] == \"<\" and leaf[-2:] == \"/>\":\n",
    "                                leaf = leaf[1:-2]  # for categorical special tokens\n",
    "                            output[key].append(leaf)\n",
    "                        if len(output[key]) == 1:\n",
    "                            output[key] = output[key][0]\n",
    "\n",
    "                tokens = tokens[tokens.find(end_token) + len(end_token) :].strip()\n",
    "                if tokens[:6] == r\"<sep/>\":  # non-leaf nodes\n",
    "                    return [output] + token2json(tokens[6:], is_inner_value=True, added_vocab=added_vocab)\n",
    "\n",
    "        if len(output):\n",
    "            return [output] if is_inner_value else output\n",
    "        else:\n",
    "            return [] if is_inner_value else {\"text_sequence\": tokens}\n",
    "\n",
    "\n",
    "def inference(test_example, model, processor, device = \"cuda\"):\n",
    "    test_image = test_example[\"image\"]\n",
    "    inputs = processor(text=PROMPT, images=test_image, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=MAX_LENGTH)\n",
    "\n",
    "    # Next turn each predicted token ID back into a string using the decode method\n",
    "    # chop of the prompt, which consists of image tokens and our text prompt\n",
    "    image_token_index = model.config.image_token_index\n",
    "    num_image_tokens = len(generated_ids[generated_ids==image_token_index])\n",
    "    num_text_tokens = len(processor.tokenizer.encode(PROMPT))\n",
    "    num_prompt_tokens = num_image_tokens + num_text_tokens + 2\n",
    "    generated_text = processor.batch_decode(generated_ids[:, num_prompt_tokens:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    print('generated_text', generated_text)\n",
    "    return generated_text[0]\n",
    "\n",
    "\n",
    "def configure_lora():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_type=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    return bnb_config, lora_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    processor, model = initialize_processor_and_model()\n",
    "    bnb_config, lora_config = configure_lora()\n",
    "    \n",
    "    # Load the model quantized model..\n",
    "    model = PaliGemmaForConditionalGeneration.from_pretrained(REPO_ID, quantization_config=bnb_config, device_map={\"\": 0})\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    config = {\"max_epochs\": 1,\n",
    "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 2,\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,} \n",
    "    \n",
    "    \n",
    "    # Load datasets\n",
    "    dataset, train_dataset, val_dataset = load_data()\n",
    "    model_module = PaliGemmaModelPLModule(config, processor, model)\n",
    "    \n",
    "    # define model checkpoint..\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints/\",  # Path to save checkpoints\n",
    "        filename=\"best_model-{epoch}-{val_edit_distance:.4f}\",  # Checkpoint filename format\n",
    "        monitor=\"val_edit_distance\",  # Metric to monitor for saving the best model\n",
    "        save_top_k=3,  # Save the top 3 models based on validation metric\n",
    "        mode=\"min\",  # Lower validation loss is better (for \"val_edit_distance\")\n",
    "        save_last=True,  # Save the last model checkpoint\n",
    "    )\n",
    "    \n",
    "    # Early stopping to prevent overfitting..\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_edit_distance\",  # Metric to monitor for early stopping\n",
    "        patience=3,  # Stop if no improvement for 3 epochs\n",
    "        mode=\"min\",  # Lower validation loss is better\n",
    "    )\n",
    "    \n",
    "    wandb_logger = WandbLogger(project=WANDB_PROJECT, name=WANDB_NAME)\n",
    "    \n",
    "    # define the trainer \n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        #devices=[0],\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision=\"16-mixed\",\n",
    "        #max_steps=-1,\n",
    "        limit_val_batches=5,\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=wandb_logger,\n",
    "        callbacks= [checkpoint_callback, early_stop_callback])\n",
    "    trainer.fit(model_module)\n",
    "    \n",
    "    \n",
    "    # Save and zip the model\n",
    "    save_directory = \"/kaggle/working/paligemma_finetuned\"\n",
    "    zip_path = save_model(model_module, save_directory)\n",
    "    print(f\"Model saved at: {zip_path}\")\n",
    "    \n",
    "    # Inference the fine-tuned model...\n",
    "    test_example = dataset[\"test\"][0]\n",
    "    generated_text = infer(test_example, model, processor)\n",
    "    print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
